{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMyr6lkgrVYAzmWVIpoEPAl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cheul0518/Competitions/blob/main/HuBMAP_HPA/CoaT_small.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm &> /dev/null\n",
        "!pip install einops &> /dev/null"
      ],
      "metadata": {
        "id": "n_oqdoryINm-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified from # https://github.com/mlpc-ucsd/CoaT/blob/main/src/models/coat.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "from timm.models.registry import register_model\n",
        "\n",
        "from einops import rearrange\n",
        "from functools import partial\n",
        "from torch import nn, einsum\n",
        "\n",
        "class LayerNorm2d(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "        self.bias = nn.Parameter(torch.zeros(dim))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, C, H, W = x.shape\n",
        "        u = x.mean(1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.eps)\n",
        "        x = self.weight[:, None, None] * x + self.bias[:, None, None] # self.weight.Size([dim]) => self.weight[:,None,None].size([dim,1,1])\n",
        "        return x\n",
        "\n",
        "# def _cfg_coat(url='', **kwargs):\n",
        "#     return {'url':url,\n",
        "#             'num_classes':1000,\n",
        "#             'input_size':(3, 224, 224),\n",
        "#             'pool_size':None,\n",
        "#             'crop_pct':.9,\n",
        "#             'interpolation':'bicubic',\n",
        "#             'mean': IMAGENET_DEFAULT_MEAN,\n",
        "#             'std': IMAGENET_DEFAULT_STD,\n",
        "#             'first_conv': 'patch_embed.proj',\n",
        "#             'classifier': 'head',\n",
        "#             **kwargs\n",
        "#             }\n",
        "class Mlp(nn.Module):\n",
        "    ''' Feed Forward Network(FFN)'''\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class ConvRelPosEnc(nn.Module):\n",
        "      \"\"\" Convolutional Relative Position Encoding\"\"\"\n",
        "      def __init__(self, Ch, h, window): # Ch=embed_dims//num_heads, h=num_heads, window=crpe_window\n",
        "          \"\"\" Intialization.\n",
        "              Ch: Channels per head\n",
        "              h: Number of heads\n",
        "              window: Window size(s) in convolutional relative positional encoding. It can have two forms:\n",
        "                  1. An integer of window size, which assigns all attention heads with the same window size in ConvRelPosEnc\n",
        "                  2. A dict mapping window size to #attention head splits\n",
        "                      (e.g. {window size 1: #attention head split 1, window_size 2: #attention head split 2})\n",
        "                      It will apply different window size to the attention head splits.\n",
        "          \"\"\"\n",
        "          super().__init__()\n",
        "\n",
        "          if isinstance(window, int):\n",
        "              window = {window: h}\n",
        "              self.window = window\n",
        "          elif isinstance(window, dict):\n",
        "              self.window = window\n",
        "          else:\n",
        "              raise ValueError()\n",
        "          \n",
        "          self.conv_list = nn.ModuleList()\n",
        "          self.head_splits = []\n",
        "          \n",
        "          for cur_window, cur_head_split in window.items(): # window = {3:2, 5:3, 7:3}\n",
        "              dilation = 1\n",
        "              padding_size = (cur_window + (cur_window-1)*(dilation-1))//2\n",
        "              cur_conv = nn.Conv2d(cur_head_split*Ch, # crpe_window.values() * embed_dims//num_heads\n",
        "                                   cur_head_split*Ch,\n",
        "                                   kernel_size=(cur_window,cur_window),\n",
        "                                   padding=(padding_size,padding_size),\n",
        "                                   dilation=(dilation,dilation),\n",
        "                                   groups=cur_head_split*Ch)\n",
        "              self.conv_list.append(cur_conv)\n",
        "              self.head_splits.append(cur_head_split) # self.head_splits = [2, 3, 3] at the end\n",
        "          self.channel_splits = [x*Ch for x in self.head_splits] # For CoaT small, self.channel_splits = [38, 57, 57]/[80,120,120]\n",
        "\n",
        "      def forward(self, q, v, size):\n",
        "          B, h, N, Ch = q.shape # q = [B, h, N(=out_H*out_W)+1, Ch], where Ch = C'(embed_dims[i])//num_heads(=8). For CoaT Small, Ch=[19,40,40,40]\n",
        "          H, W = size # out_H, out_W, where out_H,out_w=H//patch_size[0],w//patch_size[1]\n",
        "          assert N == 1 + H*W\n",
        "\n",
        "          # Convolutional relative position encoding\n",
        "          q_img = q[:,:,1:,:] # [B, h, N(=out_H*out_W), Ch]\n",
        "          v_img = v[:,:,1:,:] # [B, h, N(=out_H*out_W), Ch]\n",
        "\n",
        "          v_img = rearrange(v_img, 'B h (H W) Ch -> B (h Ch) H W', H=H, W=W) # [B, h, N, Ch] -> [B, C'(=h*Ch), out_H, out_W]\n",
        "\n",
        "          v_img_list = torch.split(v_img, self.channel_splits, dim=1) # Split according to channels. For CoaT smal, [38,57,57]/[80,120,120]\n",
        "          conv_v_img_list = [conv(x) for conv, x in zip(self.conv_list, v_img_list)] # [B, channel_splits[i], out_H, out_W], where channel_splits.sum() = Ch \n",
        "          conv_v_img = torch.cat(conv_v_img_list, dim=1) # [B, Ch, out_H, out_W]\n",
        "          conv_v_img = rearrange(conv_v_img, 'B (h Ch) H W -> B h (H W) Ch', h=h) # Shape: [B, h*Ch, out_H, out_W] -> [B, h, out_H*out_W, Ch]\n",
        "\n",
        "          EV_hat_img = q_img * conv_v_img\n",
        "          zero = torch.zeros((B, h, 1, Ch), dtype=q.dtype, layout=q.layout, device=q.device)\n",
        "          EV_hat = torch.cat((zero, EV_hat_img), dim=2) # Add cls_token\n",
        "          return EV_hat # [B, h, N+1, Ch]\n",
        "\n",
        "class FactorAtt_ConvRelPosEnc(nn.Module):\n",
        "      \"\"\" Factorized attention with convolutional relative position encoding class \"\"\"\n",
        "      def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., shared_crpe=None):\n",
        "          super().__init__()\n",
        "          self.num_heads = num_heads\n",
        "          head_dim = dim // num_heads\n",
        "          self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "          self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias)\n",
        "          self.attn_drop = nn.Dropout(attn_drop) # Not used\n",
        "          self.proj = nn.Linear(dim, dim)\n",
        "          self.proj_drop = nn.Dropout(proj_drop)\n",
        "          \n",
        "          # Shared convolutional relative position encoding\n",
        "          self.crpe = shared_crpe\n",
        "\n",
        "      def forward(self, x, size):\n",
        "          B, N, C = x.shape # [B, N(=out_H * out_W)+1, C'], where out_H,out_W = H//patch_size[0],W//patch_size[1], C'= embed_dims[i]\n",
        "          assert N == 1 + size[0] * size[1]\n",
        "\n",
        "          # Generate Q, K, V\n",
        "          qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C//self.num_heads).permute(2,0,3,1,4) # Shape: [3, B, h, N+1, Ch]\n",
        "          q, k, v = qkv[0], qkv[1], qkv[2] # Each shape: [B, h, N+1, Ch]. Note q, k, v are all different\n",
        "\n",
        "          # Factorized attention\n",
        "          k_softmax = k.softmax(dim=2)\n",
        "          k_softmax_T_dot_v = einsum('b h n k, b h n v -> b h k v', k_softmax, v) # Shape: [B, h, Ch, Ch]\n",
        "          factor_att = einsum('b h n k, b h k v -> b h n v', q, k_softmax_T_dot_v) # Shape: [B, h, N+1, Ch]\n",
        "\n",
        "          # Convolutional relative position encoding\n",
        "          crpe = self.crpe(q, v, size=size) # Shape: [B, h, N+1, Ch]\n",
        "\n",
        "          # Merge and reshape\n",
        "          x = self.scale * factor_att + crpe\n",
        "          x = x.transpose(1,2).reshape(B, N, C) # [B, N+1, C']\n",
        "\n",
        "          # Output projection\n",
        "          x = self.proj(x)\n",
        "          x = self.proj_drop(x)\n",
        "          \n",
        "          return x # [B, N+1, C']\n",
        "\n",
        "class ConvPosEnc(nn.Module):\n",
        "      \"\"\" Convolutional Postion Encoding.\n",
        "          Note: this module is similar to the conditional positional encoding in CPVT\n",
        "      \"\"\"\n",
        "      def __init__(self, dim, k=3):\n",
        "          super(ConvPosEnc, self).__init__()\n",
        "          self.proj = nn.Conv2d(dim, dim, k, 1, k//2, groups=dim) # keep the size\n",
        "      \n",
        "      def forward(self, x, size):\n",
        "          B, N, C = x.shape # B, N(=out_H*out_W) + 1(cls token), C', where out_H,out_W = H//patch_size[0],W//patch_size[1], C'= embed_dims[i]\n",
        "          H, W = size\n",
        "          assert N == 1 + H * W\n",
        "\n",
        "          # Extract cls token and image tokens\n",
        "          cls_token, img_tokens = x[:,:1], x[:,1:] # Shape: [B, 1, C'], [B, N(=out_H*out_W), C']\n",
        "\n",
        "          # Depthwise convolution\n",
        "          feat = img_tokens.transpose(1, 2).view(B, C, H, W) # [B, C', out_H, out_W]\n",
        "          x = self.proj(feat) + feat\n",
        "          x = x.flatten(2).transpose(1,2) # [B, N(=out_H*out_W), C']\n",
        "\n",
        "          # Combine with CLS token\n",
        "          x = torch.cat((cls_token, x), dim=1) # [B, N(=out_H*out_W)+1, C']. Note: cls_token must be the first param in the eq.\n",
        "          return x # [B, N(=out_H*out_W)+1, C']\n",
        "\n",
        "class SerialBlock(nn.Module):\n",
        "      \"\"\" Serial block class\n",
        "          Note: in this implementation, each serial block only contains a conv-attention and a FFN module\n",
        "      \"\"\"\n",
        "      def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                   drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, shared_cpe=None, shared_crpe=None):\n",
        "          super().__init__()\n",
        "          \n",
        "          # Conv-Attention\n",
        "          self.cpe = shared_cpe\n",
        "          self.norm1 = norm_layer(dim)\n",
        "          self.factoratt_crpe = FactorAtt_ConvRelPosEnc(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                                                        attn_drop=attn_drop, proj_drop=drop, shared_crpe=shared_crpe)\n",
        "          self.drop_path = DropPath(drop_path) if drop_path >0. else nn.Identity()\n",
        "\n",
        "          # MLP\n",
        "          self.norm2 = norm_layer(dim)\n",
        "          mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "          self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "          \n",
        "      def forward(self, x, size):\n",
        "          # Conv-Attention\n",
        "          x = self.cpe(x, size) # Apply convolutional position encoding. Shape: in&out:[B,N+1,C]\n",
        "          cur = self.norm1(x)\n",
        "          cur = self.factoratt_crpe(cur, size) # Apply factorized attention and convolutoinal position encoding. Shape:[B,N+1,C']\n",
        "\n",
        "          # MLP\n",
        "          cur = self.norm2(x)\n",
        "          cur = self.mlp(cur) # Shape:[B,N+1,C']\n",
        "          x = x + self.drop_path(cur)\n",
        "          return x\n",
        "\n",
        "class ParallelBlock(nn.Module):\n",
        "    \"\"\" Parallel block class. \"\"\"\n",
        "    def __init__(self, dims, num_heads, mlp_ratios=[], qkv_bias=False, qk_scale=None, drop=0.,attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
        "                 shared_cpes=None, shared_crpes=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # Conv-Attention\n",
        "        self.cpes = shared_cpes\n",
        "\n",
        "        self.norm12 = norm_layer(dims[1])\n",
        "        self.norm13 = norm_layer(dims[2])\n",
        "        self.norm14 = norm_layer(dims[3])\n",
        "        self.factoratt_crpe2 = FactorAtt_ConvRelPosEnc(dims[1], num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                                                       attn_drop=attn_drop, proj_drop=drop, shared_crpe=shared_crpes[1])\n",
        "        self.factoratt_crpe3 = FactorAtt_ConvRelPosEnc(dims[2], num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                                                       attn_drop=attn_drop, proj_drop=drop, shared_crpe=shared_crpes[2])\n",
        "        self.factoratt_crpe4 = FactorAtt_ConvRelPosEnc(dims[3], num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                                                       attn_drop=attn_drop, proj_drop=drop, shared_crpe=shared_crpes[3])\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "        # MLP\n",
        "        self.norm22 = norm_layer(dims[1])\n",
        "        self.norm23 = norm_layer(dims[2])\n",
        "        self.norm24 = norm_layer(dims[3])\n",
        "        assert dims[1] == dims[2] == dims[3] # In parallel block, we assume dimensions are the same and share the linear transformation.\n",
        "        assert mlp_ratios[1] == mlp_ratios[2] == mlp_ratios[3]\n",
        "        mlp_hidden_dim = int(dims[1] * mlp_ratios[1])\n",
        "        self.mlp2 = self.mlp3 = self.mlp4 = Mlp(in_features=dims[1], hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def upsample(self, x, output_size, size):\n",
        "        \"\"\" Feature map up-sampling\"\"\"\n",
        "        return self.interpolate(x, output_size=output_size, size=size)\n",
        "\n",
        "    def downsample(self, x, output_size, size):\n",
        "        \"\"\" Feature map down-sampling\"\"\"\n",
        "        return self.interpolate(x, output_size=output_size, size=size)\n",
        "    \n",
        "    def interpolate(self, x, output_size, size):\n",
        "        \"\"\" Feature map interpolation \"\"\"\n",
        "        B, N, C = x.shape\n",
        "        H, W = size\n",
        "        assert N == 1 + H*W\n",
        "\n",
        "        cls_token = x[:,:1,:]\n",
        "        img_tokens = x[:,1:,:]\n",
        "\n",
        "        img_tokens = img_tokens.transpose(1,2).reshape(B, C, H, W)\n",
        "        img_tokens = F.interpolate(img_tokens, size=output_size, mode='bilinear') # may have alignment issue\n",
        "        img_tokens = img_tokens.reshape(B, C, -1).transpose(1,2)\n",
        "\n",
        "        out = torch.cat((cls_token, img_tokens), dim=1)\n",
        "\n",
        "        return out\n",
        "    \n",
        "    def forward(self, x1, x2, x3, x4, sizes):\n",
        "        _, (H2,W2), (H3,W3), (H4,W4) = sizes\n",
        "\n",
        "        # Conv-Attention: Note: x1 is ignored\n",
        "        x2 = self.cpes[1](x2, size=(H2,W2))\n",
        "        x3 = self.cpes[2](x3, size=(H3,W3))\n",
        "        x4 = self.cpes[3](x4, size=(H4,W4))\n",
        "\n",
        "        cur2 = self.norm12(x2)\n",
        "        cur3 = self.norm13(x3)\n",
        "        cur4 = self.norm14(x4)\n",
        "        cur2 = self.factoratt_crpe2(cur2, size=(H2,W2))\n",
        "        cur3 = self.factoratt_crpe3(cur3, size=(H3,W3))\n",
        "        cur4 = self.factoratt_crpe4(cur4, size=(H4,W4))\n",
        "        upsample3_2 = self.upsample(cur3, output_size=(H2,W2), size=(H3,W3))\n",
        "        upsample4_3 = self.upsample(cur4, output_size=(H3,W3), size=(H4,W4))\n",
        "        upsample4_2 = self.upsample(cur4, output_size=(H2,W2), size=(H4,W4))\n",
        "        downsample2_3 = self.downsample(cur2, output_size=(H3,W3), size=(H2,W2))\n",
        "        downsample3_4 = self.downsample(cur3, output_size=(H4,W4), size=(H3,W3))\n",
        "        downsample2_4 = self.downsample(cur2, output_size=(H4,W4), size=(H2,W2))\n",
        "        cur2 = cur2 + upsample3_2 + upsample4_2\n",
        "        cur3 = cur3 + upsample4_3 + downsample2_3\n",
        "        cur4 = cur4 + downsample3_4 + downsample2_4\n",
        "        x2 = x2 + self.drop_path(cur2)\n",
        "        x3 = x3 + self.drop_path(cur3)\n",
        "        x4 = x4 + self.drop_path(cur4)\n",
        "\n",
        "        # MLP\n",
        "        cur2 = self.norm22(x2)\n",
        "        cur3 = self.norm23(x3)\n",
        "        cur4 = self.norm24(x4)\n",
        "        cur2 = self.mlp2(cur2)\n",
        "        cur3 = self.mlp3(cur3)\n",
        "        cur4 = self.mlp4(cur4)\n",
        "        x2 = x2 + self.drop_path(cur2)\n",
        "        x3 = x3 + self.drop_path(cur3)\n",
        "        x4 = x4 + self.drop_path(cur4)\n",
        "\n",
        "        return x1, x2, x3, x4\n",
        "        \n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\"\"\"\n",
        "    def __init__(self, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        patch_size = to_2tuple(patch_size) # e.g) 16 -> (16, 16)\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, _, H, W = x.shape\n",
        "        out_H, out_W, = H//self.patch_size[0], W//self.patch_size[1]\n",
        "        y = self.proj(x) # Shape: [B, C, H, W] -> [B, C', out_H, out_W], where C' = embed_dim\n",
        "        x = self.proj(x).flatten(2).transpose(1,2) # Shape: [B, C, H, W] - > [B, N(=out_H*out_W), C'], where C' = embed_dim\n",
        "        out = self.norm(x)\n",
        "        return out, (out_H, out_W)\n",
        "\n",
        "class CoaT(nn.Module):\n",
        "    \"\"\" Coat Class \"\"\"\n",
        "    def __init__(self, patch_size=16, in_chans=3, embed_dims=[0,0,0,0],\n",
        "                 serial_depths=[0,0,0,0], parallel_depth=0,\n",
        "                 num_heads=0, mlp_ratios=[0,0,0,0], qkv_bias=True, qk_scale=None,\n",
        "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,\n",
        "                 norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
        "                 return_interm_layers=True, out_features=None,\n",
        "                 crpe_window={3:2, 5:3, 7:3},\n",
        "                 pretrain=None,\n",
        "                 out_norm=nn.Identity, # Use nn.Identity, nn.BatchNorm2d, LayerNorm2d                 \n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "        self.return_interm_layers = return_interm_layers\n",
        "        self.pretrain = pretrain\n",
        "        self.embed_dims = embed_dims\n",
        "        self.out_features = out_features\n",
        "        # self.num_classes = num_classes is removed since this code's no longer for classification\n",
        "\n",
        "        # Patch embeddings\n",
        "        self.patch_embed1 = PatchEmbed(patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dims[0])\n",
        "        self.patch_embed2 = PatchEmbed(patch_size=2, in_chans=embed_dims[0], embed_dim=embed_dims[1])\n",
        "        self.patch_embed3 = PatchEmbed(patch_size=2, in_chans=embed_dims[1], embed_dim=embed_dims[2])\n",
        "        self.patch_embed4 = PatchEmbed(patch_size=2, in_chans=embed_dims[2], embed_dim=embed_dims[3])\n",
        "\n",
        "        # Class tokens\n",
        "        self.cls_token1 = nn.Parameter(torch.zeros(1,1,embed_dims[0]))\n",
        "        self.cls_token2 = nn.Parameter(torch.zeros(1,1,embed_dims[1]))\n",
        "        self.cls_token3 = nn.Parameter(torch.zeros(1,1,embed_dims[2]))\n",
        "        self.cls_token4 = nn.Parameter(torch.zeros(1,1,embed_dims[3]))\n",
        "\n",
        "        # Convolutional position encodings\n",
        "        self.cpe1 = ConvPosEnc(dim=embed_dims[0],k=3)\n",
        "        self.cpe2 = ConvPosEnc(dim=embed_dims[1],k=3)\n",
        "        self.cpe3 = ConvPosEnc(dim=embed_dims[2],k=3)\n",
        "        self.cpe4 = ConvPosEnc(dim=embed_dims[3],k=3)\n",
        "\n",
        "        # Convolutional relative position encodings\n",
        "        self.crpe1 = ConvRelPosEnc(Ch=embed_dims[0]//num_heads, h=num_heads, window=crpe_window)\n",
        "        self.crpe2 = ConvRelPosEnc(Ch=embed_dims[1]//num_heads, h=num_heads, window=crpe_window)\n",
        "        self.crpe3 = ConvRelPosEnc(Ch=embed_dims[2]//num_heads, h=num_heads, window=crpe_window)\n",
        "        self.crpe4 = ConvRelPosEnc(Ch=embed_dims[3]//num_heads, h=num_heads, window=crpe_window)\n",
        "\n",
        "        # Enable stochastic depth\n",
        "        dpr = drop_path_rate\n",
        "\n",
        "        # Serial Blocks 1.\n",
        "        self.serial_blocks1 = nn.ModuleList([\n",
        "            SerialBlock(dim=embed_dims[0], num_heads=num_heads, mlp_ratio=mlp_ratios[0], qkv_bias=qkv_bias, qk_scale=qk_scale, \n",
        "                        drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer, \n",
        "                        shared_cpe=self.cpe1, shared_crpe=self.crpe1) for _ in range(serial_depths[0])])\n",
        "        \n",
        "        # Serial Blocks 2.\n",
        "        self.serial_blocks2 = nn.ModuleList([\n",
        "            SerialBlock(dim=embed_dims[1], num_heads=num_heads, mlp_ratio=mlp_ratios[1], qkv_bias=qkv_bias, qk_scale=qk_scale, \n",
        "                        drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer, \n",
        "                        shared_cpe=self.cpe2, shared_crpe=self.crpe2) for _ in range(serial_depths[1])])\n",
        "\n",
        "        # Serial Blocks 3.\n",
        "        self.serial_blocks3 = nn.ModuleList([\n",
        "            SerialBlock(dim=embed_dims[2], num_heads=num_heads, mlp_ratio=mlp_ratios[2], qkv_bias=qkv_bias, qk_scale=qk_scale, \n",
        "                        drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer, \n",
        "                        shared_cpe=self.cpe3, shared_crpe=self.crpe3) for _ in range(serial_depths[2])])\n",
        "        # Serial Blocks 4.\n",
        "        self.serial_blocks4 = nn.ModuleList([\n",
        "            SerialBlock(dim=embed_dims[3], num_heads=num_heads, mlp_ratio=mlp_ratios[3], qkv_bias=qkv_bias, qk_scale=qk_scale, \n",
        "                        drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer, \n",
        "                        shared_cpe=self.cpe4, shared_crpe=self.crpe4) for _ in range(serial_depths[3])])\n",
        "\n",
        "        # Parallel Blocks.\n",
        "        self.parallel_depth = parallel_depth\n",
        "        if self.parallel_depth > 0:\n",
        "            self.parallel_blocks = nn.ModuleList([\n",
        "                ParallelBlock(dims=embed_dims, num_heads=num_heads, mlp_ratios=mlp_ratios, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                              drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr, norm_layer=norm_layer,\n",
        "                              shared_cpes=[self.cpe1,self.cpe2,self.cpe3,self.cpe4],\n",
        "                              shared_crpes=[self.crpe1,self.crpe2,self.crpe3,self.crpe4]) for _ in range(parallel_depth)])\n",
        "        \n",
        "        self.out_norm = nn.ModuleList([out_norm(embed_dims[i]) for i in range(4)])\n",
        "\n",
        "        # Initialize weights\n",
        "        trunc_normal_(self.cls_token1, std=.02)\n",
        "        trunc_normal_(self.cls_token2, std=.02)\n",
        "        trunc_normal_(self.cls_token3, std=.02)\n",
        "        trunc_normal_(self.cls_token4, std=.02)\n",
        "        self.apply(self._init_weights) # Applies the function callable to each element in the tensor, replacing each element with the value returned by callable\n",
        "        \n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0) # Fills the input Tensor with the value.\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "    \n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'cls_token1','cls_token2','cls_token3','cls_token4'}\n",
        "\n",
        "    def insert_cls(self, x, cls_token):\n",
        "        \"\"\" Insert CLS token \"\"\"\n",
        "        cls_tokens = cls_token.expand(x.shape[0],-1,-1) # [1,1,C'] -> [B,1,C']\n",
        "        x = torch.cat((cls_tokens,x),dim=1)\n",
        "        return x\n",
        "\n",
        "    def remove_cls(self, x):\n",
        "        \"\"\" Remove CLS token\"\"\"\n",
        "        return x[:, 1:, :]\n",
        "\n",
        "    def forward(self, x0):\n",
        "        B = x0.shape[0]\n",
        "\n",
        "        # Serial Blocks 1.\n",
        "        x1, (H1, W1) = self.patch_embed1(x0) # [B,N,C'], where N=(H/patch_size[0])*(W/patch_size[1]), C'=embed_dims[0]\n",
        "        cls = self.cls_token1 # [1,1,C']\n",
        "        x1 = self.insert_cls(x1, cls) # [B,N+1,C']      \n",
        "        for blk in self.serial_blocks1:\n",
        "            x1 = blk(x1, size=(H1,W1))\n",
        "        x1_nocls = self.remove_cls(x1) # [B,N,C']\n",
        "        x1_nocls = x1_nocls.reshape(B,H1,W1,-1).permute(0,3,1,2).contiguous() # [B, C', out_H, out_W]\n",
        "\n",
        "        # Serial Blocks 2.\n",
        "        x2, (H2, W2) = self.patch_embed2(x1_nocls) # [B,N',C''], where N'=(out_H/patch_size[0])*(out_W/patch_size[1]), C''=embed_dims[1]\n",
        "        cls = self.cls_token2\n",
        "        x2 = self.insert_cls(x2, cls)        \n",
        "        for blk in self.serial_blocks2:\n",
        "            x2 = blk(x2, size=(H2,W2))\n",
        "        x2_nocls = self.remove_cls(x2)\n",
        "        x2_nocls = x2_nocls.reshape(B,H2,W2,-1).permute(0,3,1,2).contiguous()\n",
        "\n",
        "        # Serial Blocks 3.\n",
        "        x3, (H3, W3) = self.patch_embed3(x2_nocls)\n",
        "        cls = self.cls_token3\n",
        "        x3 = self.insert_cls(x3, cls)\n",
        "        for blk in self.serial_blocks3:\n",
        "            x3 = blk(x3, size=(H3,W3))\n",
        "        x3_nocls = self.remove_cls(x3)\n",
        "        x3_nocls = x3_nocls.reshape(B,H3,W3,-1).permute(0,3,1,2).contiguous()\n",
        "        \n",
        "        # Serial Blocks 4.\n",
        "        x4, (H4, W4) = self.patch_embed4(x3_nocls)\n",
        "        cls = self.cls_token4\n",
        "        x4 = self.insert_cls(x4, cls)\n",
        "        for blk in self.serial_blocks4:\n",
        "            x4 = blk(x4, size=(H4,W4))\n",
        "        x4_nocls = self.remove_cls(x4)\n",
        "        x4_nocls = x4_nocls.reshape(B,H4,W4,-1).permute(0,3,1,2).contiguous()       \n",
        "\n",
        "        # Lite version\n",
        "        if self.parallel_depth == 0:\n",
        "            x1_nocls = self.out_norm[0](x1_nocls)\n",
        "            x2_nocls = self.out_norm[1](x2_nocls)\n",
        "            x3_nocls = self.out_norm[2](x3_nocls)\n",
        "            x4_nocls = self.out_norm[3](x4_nocls)\n",
        "            return [x1_nocls, x2_nocls, x3_nocls, x4_nocls]\n",
        "        \n",
        "        # Parallel blocks\n",
        "        if self.parallel_depth > 0:\n",
        "            for blk in self.parallel_blocks:\n",
        "                x1, x2, x3, x4 = blk(x1, x2, x3, x4, sizes=[(H1,W1),(H2,W2),(H3,W3),(H4,W4)])\n",
        "\n",
        "            x1_nocls = self.remove_cls(x1)\n",
        "            x1_nocls = x1_nocls.reshape(B, H1, W1, -1).permute(0,3,1,2).contiguous()\n",
        "            x1_nocls = self.out_norm[0](x1_nocls)\n",
        "\n",
        "            x2_nocls = self.remove_cls(x2)\n",
        "            x2_nocls = x2_nocls.reshape(B, H2, W2, -1).permute(0,3,1,2).contiguous()\n",
        "            x2_nocls = self.out_norm[1](x2_nocls)\n",
        "\n",
        "            x3_nocls = self.remove_cls(x3)\n",
        "            x3_nocls = x3_nocls.reshape(B, H3, W3, -1).permute(0,3,1,2).contiguous()\n",
        "            x3_nocls = self.out_norm[2](x3_nocls)\n",
        "\n",
        "            x4_nocls = self.remove_cls(x4)\n",
        "            x4_nocls = x4_nocls.reshape(B, H4, W4, -1).permute(0,3,1,2).contiguous()\n",
        "            x4_nocls = self.out_norm[3](x4_nocls)            \n",
        "\n",
        "            return [x1_nocls,x2_nocls,x3_nocls,x4_nocls]\n",
        "\n",
        "class coat_small(CoaT):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(coat_small, self).__init__(patch_size=4, \n",
        "                                         embed_dims=[152,320,320,320],\n",
        "                                         serial_depths=[2,2,2,2],\n",
        "                                         parallel_depth=6,\n",
        "                                         num_heads=8,\n",
        "                                         mlp_ratios=[4,4,4,4],\n",
        "                                         pretrain='coat_small_7479cf9b.pth',\n",
        "                                         **kwargs)\n",
        "        \n",
        "if 1:\n",
        "    x = torch.randn((2,3,800,800))\n",
        "    y = coat_small()\n",
        "    out = y(x)\n",
        "    p = [x.shape for x in out]\n",
        "    print(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD6_M5R5ITT-",
        "outputId": "0ea8e346-9c4f-437f-f2b2-33734748c322"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 152, 200, 200])\n",
            "torch.Size([2, 320, 100, 100])\n",
            "torch.Size([2, 320, 50, 50])\n",
            "torch.Size([2, 320, 25, 25])\n"
          ]
        }
      ]
    }
  ]
}